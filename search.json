[{"title":"机器学习与模式识别:手写数字识别","url":"/2023/10/26/机器学习与模式识别-手写数字识别/","content":"\n## 实验内容\n1. 数据集选择\n2. Bayes判别分类\n3. Fisher 线性判别\n4. SVM的线性与非线性分类\n5. 不同分类器之间的比较\n\n> 原始数据集上传至网盘: https://pan.baidu.com/s/1uqmJg7EGxpKR62j-Qbr1ow?pwd=jjrc\n\n## 实验数据特征提取方法\n手写数字样本。每个数字有 $50$ 张图片，选择其中 $40$ 个作为训练集，$10$ 个作为测试集。\n\n首先将含有全部特征信息的手写数字图像从坐标轴中提取出来，将提取出来的书写数字图像进行二值化处理; 将处理后的每个数字图像提取 $5\\times 5$ 块模板，每个模块中 1 值像素点与总像素点的比值就是这个模块的特征值。将所有特征值放入 $5\\times 5$ 的矩阵。设定阈值 $T = 0.05$，每块内所对应的元素白像素占有率大于 $T$ ，则该块特征取1;否则取0。\n\n选择minst手写数字数据集，因为图片尺寸为 \\(28\\times 28 \\) 不为5的倍数，使用中心裁剪法将图像裁剪为25x25的大小再进行分块操作与特征提取。\n\n数据处理(特征提取) `dataset.m` 代码如下:\n```matlab\n% ----------\n%\n% 数据集处理\n%\n% ----------\n\nfunction [train_X, train_Y, test_X, test_Y] = load_datasets(train_pc)\n    % 参数设置\n    T = 0.05;\n    kernelSize = 5;\n    imgSize = 28;\n    sub_counts = floor(imgSize / kernelSize);\n    newImgSize = kernelSize * sub_counts;\n    train_X = [];\n    train_Y = [];\n    test_X = [];\n    test_Y = [];\n    \n    for digit = 0:9\n        digitFolderPath = fullfile('./mnist', num2str(digit));\n        imageFiles = dir(fullfile(digitFolderPath, '*.png'));\n        % 读取当前数字的所有图像数据\n        images = length(imageFiles);\n        for i = 1:images\n            imgPath = fullfile(digitFolderPath, imageFiles(i).name);\n            % 读取图像\n            img = double(imread(imgPath)); \n            % 中心裁剪图像\n            croppedImg = centerCropImage(img, newImgSize);\n            % 提取图像特征\n            features = getFeatures(croppedImg, kernelSize, T);\n            %features = img(:);\n            if i <= images * train_pc\n                % 划分为训练集\n                train_X = [train_X; features'];\n                train_Y = [train_Y; digit];\n            else\n                % 划分为测试集\n                test_X = [test_X; features'];\n                test_Y = [test_Y; digit];\n            end\n        end\n    end\n    \n    end\n    \n    \n    function croppedImg = centerCropImage(img, newImgSize)\n        [rows, cols] = size(img);\n        startRow = floor((rows - newImgSize) / 2) + 1;\n        startCol = floor((cols - newImgSize) / 2) + 1;\n        croppedImg = img(startRow:startRow + newImgSize - 1, startCol:startCol + newImgSize - 1);\n    end\n    \n    function features = getFeatures(img, kernelSize, T)\n        [rows, cols] = size(img);\n        % 计算分块数目\n        numBlocksRow = round(rows / kernelSize);\n        numBlocksCol = round(cols / kernelSize);\n        % 初始化特征向量\n        features = zeros(1, numBlocksRow * numBlocksCol);\n    \n        blockIndex = 1;\n        for i = 1:numBlocksRow\n            for j = 1:numBlocksCol\n                % 计算分块的起始和结束位置\n                startRow = (i - 1) * kernelSize + 1;\n                endRow = i * kernelSize;\n                startCol = (j - 1) * kernelSize + 1;\n                endCol = j * kernelSize;\n                % 计算分块的总像素数\n                totalPixels = kernelSize * kernelSize;\n                % 计算分块内白像素的个数\n                whitePixels = sum(sum(img(startRow:endRow, startCol:endCol) >= 250));\n                \n                % 根据阈值T判断特征取值\n                if whitePixels / totalPixels > T\n                    features(blockIndex) = 1;\n                else\n                    features(blockIndex) = 0;\n                end\n                blockIndex = blockIndex + 1;\n            end\n        end\n        features = features';\n    end\n```\n\n## 朴素Bayes判别分类\n\n### 理论基础\n设 $B_i$ 表示事件: 图片为数字 $i$ ，则由 Bayes 公式有\n$$\n\\begin{align*}\nP(B_i|A) = \\frac{P(A|B_i)P(B_i)}{\\sum_{j=0}^{9} P(A|B_j)P(B_j)} \n\\end{align*}\n$$\n\n其中，$P(B_i)$ 在这里是先验概率，在这里等于 0.1。$P(B_i|A)$ 是后验概率，在这里是对于一张手写数字图片(事件 $A$)上的数字是 $d$ ($0\\sim 9$ 对应事件 $B_0\\sim B_9$)的概率，由于这里是设计基于最小错误率的贝叶斯分类器，故而认为该数字为后验概率最大的数字。\n\n令 $\\mathbf{X}$ 表示图片集合，$\\mathbf{Y}$ 表示标签集合，则训练数据集可以表示为:\n$$\n\\mathbf{T} = \\{(\\bar{x}_1, y_1), (\\bar{x}_2,y_2),\\cdots, (\\bar{x}_n,y_n)\\}\n$$\n其中，$\\bar{x}_1,\\cdots,\\bar{x}_n\\in \\mathbf{X}$ , $y_1,\\cdots,y_n\\in \\mathbf{Y}$ ，对于任意的 $\\bar{x}_i$ 有 $\\bar{x}_i = \\{x_i^1, x_i^2, \\cdots, x_i^m\\}$ 意为第 $i$ 张图片的 $m$ 个特征。\n\n对于训练数据集 $P(X,Y)$ 独立同分布，所以有\n$$\nP(X|Y) = \\frac{P(X,Y)}{P(Y)}\n$$\n又有先验概率 $P(Y = c_k) = 0.1$ , $k=0,1,\\cdots,9$ , 而条件概率\n$$\nP(X=x|Y=c_k) = P(X^1=x^1,X^2=x^2,\\cdots,X^m=x^m|Y=c_k)\n$$\n又因为这里数据的条件概率分布是特征条件独立，所以进一步地可以表示为\n$$\nP(X=x|Y=c_k) = \\prod_{j=1}^m P(X^j=x^j|Y=c_k)\n$$\n\n> 在这里问题里的实际含义是: 对于测试集的任意一张 $28\\times 28$ 大小的手写数字图片，最后得到 25 个特征，每一个特征对应于每一个模块的取值。在朴素贝叶斯的假设条件下，这张图片是 1 的概率就是每一个特征都是 1 的特征的概率的累乘。\n\n进一步得到后验概率的计算公式:\n$$\nP(Y=c_k|X=x) = \\frac{P(Y=c_k)\\prod_j^m P(X^j=x^j|Y=c_k)}{\\sum_kP(Y=c_k)\\prod_j^m P(X^j=x^j|Y=c_k)}\n$$\n\n因为是要取最大值，所以可以去掉公分母，得到朴素贝叶斯分类器的判别式\n$$\ny = f(x) = \\max_{c_k} P(Y=c_k)\\prod_j P(X^j=x^j|Y=c_k)\n$$\n\n对每一个实验样本，选取前 40 个作为训练集，后 10 个作为测试集。\n\n编写代码 `bayesClassifier.m`\n```matlab\n% ----------\n%\n% 贝叶斯判别代码\n%\n% ----------\n\nclc, clear;\n% 重新加载数据\nload('datasets.mat');\n\n% v = mnistData{8}{1, 2};\n% A = reshape(v, 5, 5)';\n% disp(A);\n\n% 从每个类别中选择40个样本作为训练集，10个样本作为测试集\ntrain_samples = cell(10, 40);\ntest_samples = cell(10, 10);\n\nerror_count = 0;\nfor digit = 1:10\n    % 从当前类别中随机选择40个样本作为训练集\n    all_samples = mnistData{digit};\n%     random_indices = randperm(length(all_samples), 40);\n%     % disp(random_indices);\n%     train_samples{digit} = all_samples(random_indices, :);\n    \n    train_samples{digit} = all_samples(1:40, :);\n    % 剩余的10个样本作为测试集\n%     test_indices = setdiff(1:length(all_samples), random_indices);\n%     test_samples{digit} = all_samples(test_indices, :);\n    test_samples{digit} = all_samples(41:50, :);\n    %(test_indices);\nend\n\n% 重新计算先验概率和类条件概率\nnum_classes = 10; % 数字类别数量\nnum_features = 25; % 特征数量\nnum_train = 40; % 训练样本的数量\nnum_test = 10; % 测试样本的数量\n\ntrue_positives = zeros(1, num_classes); % 正类别被正确分类的样本数量\nfalse_positives = zeros(1, num_classes); % 负类别被错误分类成正类别的样本数量\nfalse_negatives = zeros(1, num_classes); % 正类别被错误分类成负类别的样本数量\n\nfor x = 1:10\n    for y = 1:10\n        % 获取第i个测试样本的特征向量\n        test_sample = test_samples{x}{y, 2};\n        \n        prior_prob = zeros(1, num_classes); % 先验概率\n        class_cond_prob = zeros(num_features, num_classes); % 类条件概率\n        pij = []; % i类的样本第j个特征为1的概率\n        for i = 1:num_classes\n            % 计算先验概率\n            prior_prob(i) = 0.1;\n            for j = 1:num_features % 每个数字图片提取出来的特征数\n                sum = 0;\n                for k = 1:num_train % 每个类别下训练样本的个数\n                    i_feature = train_samples{i}{k, 2}; % 获取第k个训练样本的特征向量\n                    sum = sum + i_feature(j);\n                end\n                % disp(sum);\n                pij(i,j) = (sum + 1) / (num_train + 2); % 计算概率估计值即Pj(ωi)，注意拉普拉斯平滑处理\n            end\n        end\n        for i = 1:num_classes\n            multi = 1;\n            for j = 1:num_features % 每个数字图片提取出来的特征数\n                if(test_sample(j) == 1)\n                    multi = multi * pij(i,j);\n                else\n                    multi = multi * (1 - pij(i,j));\n                end\n            end\n            class_cond_prob(i) = multi;\n        end\n        %计算后验概率\n        p_class = []; % 后验概率\n        sum = 0;\n        for i=1:num_classes%数字类别个数\n            sum = sum + prior_prob(i) * class_cond_prob(i);\n        end\n        for i = 1:num_classes % 数字类别个数\n            p_class(i) = prior_prob(i) * class_cond_prob(i) / sum;\n        end\n        [maxval, maxpos] = max(p_class);\n        if maxpos == x\n            true_positives(x) = true_positives(x) + 1;\n        else\n            error_count = error_count + 1;\n            false_positives(maxpos) = false_positives(maxpos) + 1;\n            false_negatives(x) = false_negatives(x) + 1;\n        end\n    end\nend\n\n% 计算准确率（acc）、精确率（precision）、召回率（recall）、F1-score\nprecision = true_positives ./ (true_positives + false_positives);\nrecall = true_positives ./ (true_positives + false_negatives);\nf1_score = 2 * (precision .* recall) ./ (precision + recall);\n\n% 计算错误率和正确率\nerror_rate = error_count / 100;\naccuracy = 1 - error_rate;\n\ndisp(['Accuracy: ', num2str(accuracy)]);\ndisp(['Precision: ', num2str(mean(precision))]); % Changed here\ndisp(['Recall: ', num2str(mean(recall))]); % Changed here\ndisp(['F1 Score: ', num2str(mean(f1_score))]); % Changed here\n```\n\n得到结果:\nAccuracy: 0.66\nPrecision: 0.67991\nRecall: 0.66\nF1 Score: 0.65476\n\n\n## Fisher判别分类\n使用 Fisher 线性判别方法求分类器的步骤:\n1. 计算各类的均值向量: $\\mu_i = \\frac{1}{N_i}\\sum_{x\\in X_i}x$ ;\n2. 计算各类的类内离散矩阵: $S_{wi} = \\sum_{x\\in X_i}(x-\\mu_i)(x-\\mu_i)^T$ ;\n3. 计算类内总离散矩阵: $S_w = S_{w0}+S_{w1}+\\cdots$ ;\n4. 计算总离散矩阵的逆矩阵: $S_w^{-1}$ ;\n5. 求出向量 $w^* = S_w^{-1}(\\mu_1-\\mu_0)$ ;\n6. 判别函数为: $y=(w^*)^Tx$ ;\n7. 求出判别函数的阈值: $w_0 = \\frac{(w^*)^T(\\mu_0+\\mu_1+\\cdots)}{2}$ ;\n8. 比较 $y$ 值与阈值的大小得出分类。\n\n```matlab\n% ----------\n%\n% Fisher分类代码\n%\n% ----------\n\nclc,clear;\nload('datasets.mat');\n% 初始化\n\nnumClasses = 10; % 类别数\nnumImages = 50; % 每个类别的图像数\nnumFeatures = 25; % 特征数\ntrainingSize = 40; % 训练集大小\n\n% 创建训练集和测试集\ntrainingData = zeros(numClasses * trainingSize, numFeatures);\ntrainingLabels = zeros(numClasses * trainingSize, 1);\ntestData = zeros(numClasses * (numImages - trainingSize), numFeatures);\ntestLabels = zeros(numClasses * (numImages - trainingSize), 1);\n\nfor i = 1:numClasses\n    for j = 1:numImages\n        if j <= trainingSize\n            trainingData((i-1)*trainingSize + j, :) = mnistData{i,1}{j,2};\n            trainingLabels((i-1)*trainingSize + j) = i;\n        else\n            testData((i-1)*(numImages - trainingSize) + j - trainingSize, :) = mnistData{i,1}{51-j,2};\n            testLabels((i-1)*(numImages - trainingSize) + j - trainingSize) = i;\n        end\n    end\nend\n\n% 使用Fisher线性判别方法进行训练\nMdlLinear = fitcdiscr(trainingData, trainingLabels, 'DiscrimType', 'pseudoLinear');\n% 对测试集进行预测\n\npredictedLabels = predict(MdlLinear, testData);\n\n% 计算错误率\nerrorRate = sum(predictedLabels ~= testLabels) / length(testLabels);\nfprintf('Error Rate: %.2f%%\\n', errorRate * 100);\n\n% 初始化\nprior = ones(1, numClasses) / numClasses; % 先验概率\n\n% 使用Fisher线性判别方法进行训练\nMdlLinear = fitcdiscr(trainingData, trainingLabels, 'DiscrimType', 'pseudoLinear', 'Prior', prior);\n\n% 对测试集进行预测\npredictedLabels = predict(MdlLinear, testData);\n\n% 计算错误率\nerrorRate = sum(predictedLabels ~= testLabels) / length(testLabels);\nfprintf('Error Rate: %.2f%%\\n', errorRate * 100);\n\n% 计算混淆矩阵\nC = confusionmat(testLabels, predictedLabels);\n\n% 计算准确率（accuracy）\naccuracy = sum(diag(C)) / sum(C(:));\nfprintf('Accuracy: %.2f%%\\n', accuracy * 100);\n\n% 计算精确率（precision）\nprecision = diag(C) ./ sum(C, 2);\nfprintf('Precision: %.2f%%\\n', mean(precision) * 100);\n\n% 计算召回率（recall）\nrecall = diag(C) ./ sum(C, 1)';\nfprintf('Recall: %.2f%%\\n', mean(recall) * 100);\n\n% 计算F1-score\nf1score = 2 * (precision .* recall) ./ (precision + recall);\nfprintf('F1-score: %.2f%%\\n', mean(f1score) * 100);\n```\n\n## 多分类支持向量机\n### 二分类支持向量机介绍\n对于线性不可分情况引入惩罚因子 $C$ ，于是广义最优分类面问题模型如下:\n$$\n\\max_{a} \\sum_{j=1}^N a_j - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N} y_iy_ja_ia_jK(x_i,x_j) , s.t.\\sum_{ i=1}^N y_ia_i=0\n$$\n\n其中 $0\\le a_i\\le C$ 。\n\n### 特征选择\n用于训练SVM的特征使用的是图像的完整像素特征，即一张 $28\\times 28$ 的图像，它的特征向量的大小为 $1\\times 784$ 。将该特征进行标准化处理后即可用于训练SVM。\n\n### ECOC编码与多分类SVM\nECOC(Error-Correcting Output Codes)编码是一种纠错输出编码用于将多分类任务高效地转换为多个二分类任务。Mnist数据集有0~9个数字共10分类。对应的ECOC编码如下图:\n\n![Alt text](../img/ecoc.png)\n\n代码如下:\n```matlab\n% ----------\n%\n% 支持向量机代码\n%\n% ----------\n\nclc,clear;\n\n[train_X, train_Y, test_X, test_Y] = load_datasets(0.8);\n\n% 核函数选择，可选：'linear','gaussian','rbf','polynomial'\nKernelFunction = 'polynomial';\n% 惩罚参数C确认\nC = 1000;\n\ntemplate = templateSVM(...\n    'KernelFunction', KernelFunction, ...\n    'PolynomialOrder', 3, ...\n    'KernelScale', 'auto', ...\n    'BoxConstraint', C, ...\n    'Standardize', true);\nsvm_model = fitcecoc(...\n    train_X, ...\n    train_Y, ...\n    'Learners', template);\n\n% spy(svm_model.BinaryY(1:40:400,:));\n% title('ECOC编码');\n% yticks(1:10);\n% yticklabels(0:9);\n% xlabel('分类器数目');\n\npredicted_labels = predict(svm_model, test_X);\n\n% 计算混淆矩阵\nC = confusionmat(test_Y, predicted_labels);\n\n% 计算准确率（accuracy）\naccuracy = sum(diag(C)) / sum(C(:));\nfprintf('Accuracy: %.2f%%\\n', accuracy * 100);\n\n% 计算精确率（precision）\nprecision = diag(C) ./ sum(C, 2);\nfprintf('Precision: %.2f%%\\n', mean(precision) * 100);\n\n% 计算召回率（recall）\nrecall = diag(C) ./ sum(C, 1)';\nfprintf('Recall: %.2f%%\\n', mean(recall) * 100);\n\n% 计算F1-score\nf1score = 2 * (precision .* recall) ./ (precision + recall);\nfprintf('F1-score: %.2f%%\\n', mean(f1score) * 100);\n```\n\n下表为使用全部特征进行训练、测试得到的结果。\n\n| 核函数 | 惩罚参数C | 准确率(%) | 精确率(%) | 召回率(%) | F1-score(%) |\n| :----: | :----: | :----: | :----: | :----: | :----: | \n| 线性 | 1 | 86.00 | 86.00 | 87.83 | 85.77 |\n| 线性 | 100 | 82.00 | 82.00 | 84.35 | 81.44 |\n| 线性 | 1000 | 82.00 | 82.00 | 84.35 | 81.44 |\n| 高斯 | 1 | 71.00 | 71.00 | 83.30 | 73.64 |\n| 高斯 | 100 | 72.00 | 72.00 | 82.89 | 73.80 |\n| 高斯 | 1000 | 72.00 | 72.00 | 82.89 | 73.80 |\n| 三次多项式 | 1 | 87.00 | 87.00 | 87.18 | 86.58 |\n| 三次多项式 | 100 | 87.00 | 87.00 | 87.18 | 86.58 |\n| 三次多项式 | 1000 | 87.00 | 87.00 | 87.18 | 86.58 |\n\n由上表得，三次多项式作为核函数效果最佳，且乘法参数C取值对评估结果没有影响。但如果选取原始特征提取方法(图像被分为5x5个块，一共提取了25个特征)，准确率将有所下降，仅能达到 60% 左右。\n\n## 各分类器间的比较\n无论是贝叶斯判别还是Fisher分类，两者改为多分类方法比较容易。SVM是一个性能很好的二分类算法，然而在进行多分类任务时需要多个SVM才能进行，这导致SVM在多分类任务中的准确率下降。本次实验如果全部使用提取特征后的数据来训练模型，Fisher判别表现最好，准确率在74%；其次是贝叶斯判别，准确率在66%. SVM分类效果最差，准确率为61%.\n     \n总的来说，分类算法的选择更多取决于数据集。如果数据集规模较大，且基本线性可分，使用贝叶斯或是Fisher判别效率更高，反之应使用SVM处理更加复杂的非线性分类任务。","tags":["Bayes","Fisher","SVM","matlab"]},{"title":"C++面经Part1","url":"/2023/10/25/C-面经Part1/","content":"### const\n主要有以下作用:\n1. 修饰变量，说明该变量不可改变;\n2. 修饰指针，这里分为**指向常量的指针(指针常量，pointer to const)**和**自身是常量的指针(常量指针，const pointer)**\n```cpp\nint a = 0, b = 10;\nconst int *p1 = &a; // 指针常量，指向的值不可以使用*p修改，*p = 10 错误，可以修改指向的地址\nint *const p2 = &a; // 常量指针，指针指向的地址不可改变，p2 = &b 错误，可以修改该地址的变量值\n```\n3. 修饰引用，指向常量的引用，用于形参类型，既避免了拷贝，又避免了函数对值进行修改\n4. 类内修饰成员函数，说明该成员函数内不能修改成员变量\n\n```cpp\n#include <iostream>\n\nclass A {\nprivate:\n    const int a;\npublic:\n    // 构造函数\n    A() : a(0) { };\n    A(int x) : a(x) { };\n\n    // const可用于对重载函数的区分\n    // 普通成员函数\n    void func() {\n        std::cout << \"这是一个普通函数\" << std::endl;\n    }\n    // 常成员函数，不得修改类中任何数据的值\n    void func() const {\n        std::cout << \"这是一个常函数\" << std::endl;\n    }\n};\n\nint main() {\n    A obj1;\n    const A obj2;\n    obj1.func(); // \"这是一个普通函数\"\n    obj2.func(); // \"这是一个常函数\"\n    return 0;\n}\n```\n需要注意的是，普通对象可以调用普通函数也可以调用常函数，前者优先，如果是常量对象的话，只能调用常函数，如果调用普通函数会导致报错。\n\n进一步地\n```cpp\n// 函数\nvoid function1(const int Var);           // 传递过来的参数在函数内不可变\nvoid function2(const char* Var);         // 参数指针所指内容为常量\nvoid function3(char* const Var);         // 参数指针为常量\nvoid function4(const int& Var);          // 引用参数在函数内为常量\n// 没有 const reference，因为引用只是对象的别名，引用不是对象，不能用 const 修饰\n\n// 函数返回值\nconst int function5();      // 返回一个常数\nconst int* function6();     // 返回一个指向常量的指针变量，使用：const int *p = function6();\nint* const function7();     // 返回一个指向变量的常指针，使用：int* const p = function7();\n```\n\n### static\n主要有以下作用:\n1. **修饰普通变量:** 修改变量的存储区域和生命周期，使变量存储在静态区域，在main函数运行前就分配了空间，如果有初始值就用初始值初始化它，如果没有初始值系统用默认值初始化它。需要注意的是: 在函数内部定义了一个静态变量，生命周期到程序结束，但是这个变量的作用域仅限于声明它的函数内部。\n```cpp\nvoid myFunction() {\n    static int count = 0; // 静态局部变量\n    count++;\n    std::cout << \"Count: \" << count << std::endl;\n}\n\nint main() {\n    myFunction(); // 输出 Count: 1\n    myFunction(); // 输出 Count: 2\n    // 这里无法直接访问 count\n    return 0;\n}\n```\n2. **修饰普通函数:** 表明函数的作用范围，仅在定义该函数的文件内才能使用，它的作用域被限制在声明它的文件中，即它变成了一个“内部链接”的函数，只能在当前文件内部访问。在多人开发项目时，为了防止与他人命名空间里的函数重名，可以将函数定位为 static。\n```cpp\n// File1.cpp\nstatic void myFunction() {\n    std::cout << \"这是一个静态函数\" << std::endl;\n}\n// File2.cpp\nvoid anotherFunction() {\n    myFunction(); // 错误，无法访问静态函数\n}\n```\n3. **修饰成员变量:** 修饰成员变量时该变量将被所有该类的对象共享，而不是每个对象拥有一份副本，而且不需要生成对象就可以访问该成员。\n```cpp\nclass MyClass {\npublic:\n    static int count; // 静态成员变量\n};\n\nint MyClass::count = 0; // 静态成员变量的初始化\n\nint main() {\n    MyClass obj1;\n    MyClass obj2;\n\n    obj1.count = 5;\n    std::cout << obj2.count << std::endl; // 输出 5\n\n    return 0;\n}\n```\n4. **修饰成员函数:** 修饰成员函数使得不需要生成对象就可以访问该函数，但是在 static 函数内只能访问 static 成员。\n\n### this 指针\n`this` 指针是一个特殊的指针，它指向当前对象的地址。在 C++ 中，每个类的非静态成员函数都有一个隐含的 `this`` 指针，它指向调用该成员函数的对象。\n\n当一个类对象调用成员函数时，编译程序先将对象的地址赋给了 `this` 指针，然后调用该成员函数，每次成员函数存取数据成员时，都其实是在隐式地使用 `this` 指针。\n\n`this` 指针是一个常量指针，被隐含地声明为: `ClassName *const this` ，这意味着不能给 `this` 指针赋值，而在 `const` 成员函数里被声明为 `const ClassName* const`\n\n最后还需要注意的是，`this` 并不是一个常规变量，而是个右值，所以不能取得 `this` 的地址(不能 `&this`)\n\n### 左值和右值(引用)\n1. 左值(lvalue)\n+ 左值既能够出现在等号左边，也能出现在等号右边\n+ 左值可以被赋值，可以作为赋值语句的目标\n+ 左值是可寻址的变量，有持久性\n+ 具体来说，变量、对象或者通过解引用获得的指针都属于左值\n```cpp\nint x = 5; // x 是左值，因为它代表一个内存位置，可以被赋值\nint* ptr = &x; // &x 是左值，因为它是变量 x 的地址\n```\n\n2. 右值(rvalue)\n+ 右值是不能被赋值的表达式，它们代表的是一个数值或者临时值，通常在赋值语句的右侧\n+ 右值可以是一个常数、一个临时的计算结果或者一个表达式的返回值。\n+ 右值在使用后就失去意义，因此不能被取地址\n```cpp\nint y = 10; // 10 是右值，因为它代表一个数值，不能被赋值\nint z = x + y; // x + y 是右值，因为它代表一个临时计算结果\n```\n\n> C++ Prime:\" 当一个对象被用作右值的时候，用的是对象的值(内容);当对象被用作左值时，用的是对象的身份(在内存中的位置)\"\n\n左值引用(lvalue reference)和右值引用(rvalue reference)是C++中引入的两种不同类型的引用。\n\n3. **左值引用:** 左值引用是最常见的引用类型。它们使用 `&` 符号声明，并且只能绑定到左值(可以取地址的表达式)。\n4. **右值引用:** 右值引用是在C++11中引入的新特性，用 && 符号表示。它们可以绑定到临时值、表达式结果或具有名称的右值。\n```cpp\nint x = 6; // x是左值，6是右值\nint &y = x; // 左值引用，y引用x\n\nint &z1 = x * 6; // 错误，x*6是一个右值\n\nint &&z2 = x * 6; // 正确，右值引用\nint &&z3 = x; // 错误，x是一个左值\n```\n> 可以引用右值的除了右值引用外还有 const 左值引用，例如 `const int &z4 =  x * 6;` 正确，可以将一个const引用绑定到一个右值。例如 `std::vector` 的 `push_back` 函数就使用了 const 左值引用(`void push_back (const value_type& val);`) ，这样能让我们使用 `v.push_back(1)` 这样的代码。\n\n`std::move` 可以实现将左值转换成右值以实现对左值进行右值引用\n```cpp\nint i = 3, j;\nj = std::move(2); // 合法，从一个右值移动数据\nj = std::move(i); // 合法，从一个左值移动数据，i的值之后是不确定的。\n```\n\n### inline 内联函数\n主要有以下特征:\n+ 相当于把内联函数里面的内容写在调用内联函数处;\n+ 相当于不用执行进入函数的步骤，直接执行函数体;\n+ 相当于宏，却比宏多了类型检查，真正具有函数特性;\n+ 编译器一般不内联包含循环、递归、switch 等复杂操作的内联函数;\n+ 在类声明中定义的函数，除了虚函数的其他函数都会自动隐式地当成内联函数。\n\n```cpp\n// 一般在类内定义的成员函数会隐式成为inline，而在类外定义的成员函数不会(当然是否内联完全取决于编译器)\n// 类内定义，隐式内联\nclass A {\n    int doA() { return 0; }         // 隐式内联\n}\n// 类外定义，需要显式内联\nclass A {\n    int doA();\n}\ninline int A::doA() { return 0; }   // 需要显式内联\n```\n\n虚函数可以是内联函数吗？\n[Standard C++: Are “inline virtual” member functions ever actually “inlined”?](https://isocpp.org/wiki/faq/value-vs-ref-semantics#:~:text=Therefore%20the%20only%20time%20an,or%20reference%20to%20an%20object.)\n[Stackoverflow: Can virtual functions be inlined [duplicate]](https://stackoverflow.com/questions/18432040/can-virtual-functions-be-inlined)\n\n虚函数可以是内联函数，但是当虚函数表现出多态性的时候不能内联，因为内联是在编译期建议编译器内联，而虚函数的多态性在运行期，编译器无法知道运行期调用哪个代码，因此虚函数表现为多态性的时候不可以内联。\n\n下面是虚函数内联使用例子\n```cpp\n#include <iostream>\n\nclass Base {\npublic:\n    inline virtual void who() {\n        std::cout << \"I am Base\" << std::endl;\n    }\n    virtual ~Base() { }\n};\n\nclass Derived: public Base {\npublic:\n    // // 不写 inline 时会隐式内联\n    inline void who() {\n        std::cout << \"I am Derived\" << std::endl;\n    }\n};\n\nint main() {\n    Base fa;\n    fa.who(); // 此处的虚函数 who()，是通过类（Base）的具体对象（b）来调用的，编译期间就能确定了，所以它可以是内联的，但最终是否内联取决于编译器。\n\n    // 此处的虚函数是通过指针调用的，呈现多态性，需要在运行时期间才能确定，所以不能为内联。\n    Base *ptr = new Derived();\n    ptr->who();\n\n    // 因为Base有虚析构函数（virtual ~Base() {}），所以 delete 时，会先调用派生类（Derived）析构函数，再调用基类（Base）析构函数，防止内存泄漏。\n    delete ptr;\n    ptr = nullptr;\n\n    return 0;\n}\n```\n\n具体来说，就是\n1. 当使用类的对象来调用时，则虚函数可以当做是内联的，因为编译器在编译时就确切知道对象是哪个类的;\n2. 当使用基类指针或引用来调用虚函数时，它都不能是内联函数，因为调用发生在运行时，是动态绑定的。\n","tags":["C++"]},{"title":"CSAPP-计算机系统漫游","url":"/2023/10/23/CSAPP-计算机系统漫游/","content":"之前在读 xv6-book 的时候，发现自己对一些计算机底层的知识了解的太少，导致看到虚拟内存映射、物理地址这些东西的时候就很头大。在一个群友的推荐下，我决定先来看一下CSAPP的1、2、3、6、7、8、9章(当然，还有一种方案是学习CS61C)，于是有了这个系列。\n\ngitbook: https://hansimov.gitbook.io/csapp/\n\n## 系统的硬件组成\n### 1.总线\n总线是贯穿整个系统的一组电子管道。它们携带者信息字节在各个部件之间传递，传送定长的字节块(字)。字中的字节数(字长)是一个基本的系统参数，通常是4个字节(32位)、8个字节(64位)。\n\n### 2.I/O 设备\n示例有作为用户输入的键盘和鼠标，作为用户输出的显示器，用于长期存储数据和程序的磁盘驱动器(磁盘)。每个 I/O 设备都通过一个**控制器**或者**适配器**与 I/O 总线相连。\n> 控制器和适配器之间的区别在于它们的封装方式: 控制器是 I/O 设备本身或系统的主板上的芯片组，而适配器则是一块插在主板插槽上的卡。功能都是在 I/O 总线和 I/O 设备之间传递信息。\n\n### 3.主存\n主存是一个临时存储设备，在处理器执行程序时，用来存放程序和程序处理的数据。从物理上来说，主存是由一组 **动态随机获取存储器(DRAM)** 芯片组成;从逻辑上来说，存储器是一个线性的字节数组，每个字节都有其唯一的地址，这些地址是从零开始的。\n\n### 4.处理器\n**中央处理单元(CPU)**简称为处理器，是解释(或执行)存储在主存中指令的引擎。处理器的核心是一个大小为一个字的存储设备(或**寄存器**)，称为**程序计数器(PC)**。在任何时刻，PC都指向主存中的某条机器语言指令(该指令的地址)\n\n处理器从通电到系统断电一直在不断从程序计数器指向的内存处读取指令，解释指令中的位，执行该指令指示的简单操作，然后更新 PC，使其指向下一条指令。\n\n这样的操作围绕着主存、寄存器文件(register file)和算数/逻辑单元(ALU)进。寄存器文件是一个小的存储设备，由一些单个字长的寄存器组成，每个寄存器都有唯一的名字。ALU计算新的数据和地址值。\n\nCPU在指令的要求下可能会执行如下操作:\n+ **加载:** 从主存复制一个字节或者一个字到寄存器，以覆盖寄存器原来的内容;\n+ **存储:** 从寄存器复制一个字节或者一个字到主存的某个位置，以覆盖这个位置上原来的内容;\n+ **操作:** 把两个寄存器的内容复制到ALU，ALU对这两个字做算数运算，并将结果存放到一个寄存器中;\n+ **跳转:** 从指令本身抽取一个字，并将这个字复制到PC中，以覆盖PC中原先的值。\n\n## 操作系统管理硬件\n操作系统有两个基本功能∶ \n1. 防止硬件被失控的应用程序滥用;\n2. 向应用程序提供简单一致的机制来控制复杂而又通常大不相同的低级硬件设备。\n\n操作系统通过几个基本的抽象概念(**进程、虚拟内存和文件**)来实现。\n\n> 文件是对 I/O 设备的抽象表示，虚拟内存是对主存和磁盘 I/O 设备的抽象表示，进程则是对处理器、主存和 I/O 设备的抽象表示。\n\n### 进程\n进程是操作系统对一个正在运行的程序进行的一种抽象。一个CPU在同一时间只能执行一个进程，但是可以**并发运行**，即一个进程的指令和另一个进程的指令是交错执行，操作系统实现这种交错执行的机制称为**上下文切换**，而上下文就是操作系统保持跟踪进程运行所需的所有状态信息。\n\n### 线程\n在现代操作系统中，一个进程实际上可以由多个称为**线程**的执行单元组成，每个线程都运行在进程的上下文中，并共享同样的代码和全局数据。\n\n### 虚拟内存\n使用 xv6 里的解释\n\n虚拟内存是每个进程都会有自己独立的 page table，每一个进程只能访问出现在自己page table中的物理内存。操作系统会设置page table，使得每一个进程都有不重合的物理内存，这样一个进程就不能访问其他进程的物理内存，因为其他进程的物理内存都不在它的 page table 中。\n> 就比如 ls 程序会有一个内存地址 0，echo 程序也会有一个内存地址 0，但是操作系统会将两个程序的内存地址0映射到不同的物理内存地址，所以ls程序不能访问echo程序的内存，同样echo程序也不能访问ls程序的内存。\n\n### 文件\n文件就是字节序列。每个I/O设备，包括磁盘、键盘、显示器，甚至网络，都可以看成是文件。系统中的所有输入输出都是通过使用一小组称为 Unix I/O 的系统函数调用读写文件来实现的。\n\n## 系统之间利用网络通信\n从一个单独的系统来看，网络可视为一个 I/O 设备，当系统从主存复制一串字节到网络适配器时，数据流经过网络到达另一台机器，相似地，系统可以读取从其他机器发送来的数据，并把数据复制到自己的主存。","tags":["CSAPP"]},{"title":"微信Robot使用手册","url":"/2023/10/22/微信Robot使用手册/","content":"\n> bot wechat id: liuwx_robot\n\n该机器人依赖于 [chatgpt-on-wechat](https://github.com/zhayujie/chatgpt-on-wechat) 项目进行开发\n\n主要在私聊和群聊中使用，私聊需要使用 bot 作为前缀，比如发送:\"bot 你是谁？\"，如果没有前缀将会被视为普通消息而不予回复。群聊直接 @ 即可。\n\n## 角色功能如下:\n\n以下示例指令都使用 `bot` 为前缀，这是在私聊场景中的使用方法，在群聊中，你只需要将 `bot` 前缀更换为 @ bot 即可。\n\n### 1.Chatgpt3.5\nbot 接入了 chatgpt3.5 ，可以回复你想问的任何问题，即你可以直接将该 bot 当作一个不需要挂代理的 chatgpt 使用，需要注意的是，问题回复的时间将由此时的网络代理环境和回复答案的字数而定，请耐心等待。\n\n### 2.语音识别\nbot 可以进行语音识别，你可以直接发送语音消息进行提问，bot 会对私聊和群聊中所有的语音消息进行识别并且作出回复，需要注意的是，你应该尽量使用标准的普通话。\n\n### 3.AI绘画\nbot 接入了基于 LinkAI 提供的 Midjourney 绘画，提问时只需要以为 \"画\" 为开头即可完成绘画，比如在私聊场景下你可以发送 \"bot 画一只青蛙\" 来进行绘画。\n\n### 4.文档总结\nbot 接入了基于 LinkAI 提供的文档总结对话，你向机器人发送 **文件** 或 **分享链接卡片** 即可生成摘要，进一步可以与文件或链接的内容进行多轮对话。\n> 值得注意的是，目前 bot 仅支持 txt, docx, pdf, md, csv 格式的文件，同时文件大小不能超过 5M ，文件字数最多可支持百万字的文件。分享链接 目前仅支持 公众号文章，后续会支持更多文章类型及视频链接等。\n\n### 5.角色预设\nbot 可以进行角色设定，具体使用方法以及可用角色设定可以使用 `bot $role help` 即可进行查看。\n\n### 6.冒险游戏\n使用 `bot $开始冒险 <故事背景>` ，如果不填写故事背景则会使用默认故事背景，bot 回复后你需要继续告诉 bot 后续发展，这样在你的引导下 bot 会慢慢完善这个完整的冒险游戏。\n\n### 7.管理员指令\n可以根据 `@bot #help` 进行查看，设置管理员需要在私聊中设置，具体方法为 `#auth <口令>` ，口令仅有我本人知晓。\n\n目前该机器人部署在我的个人电脑上，因此仅在开机时可以使用，后续会考虑将其部署至服务器上。\n\n该 bot 使用日期为即日起至 bot 被腾讯风控或者本人无力继续维护时，且用且珍惜。","tags":["Robot"]},{"title":"Hello World","url":"/2023/10/19/hello/","content":"\n> 你好, 世界!\n\n本博客搭建于2023年10月18日，使用 github pages + hexo 进行搭建，使用 aircloud 主题，这是一个简洁轻量的 hexo 博客主题。\n\n这是笔者第三次搭建博客，第一次也是使用 hexo 搭建的一个静态博客，可惜后来在维护过程中遇到了一些当时无法解决的问题，于是放弃。第二次是使用腾讯云服务器，基于 wordpress 框架搭建的一个动态网站，还给它取了一个响亮的名字 —— 「牛牛网」，只是可惜在使用几个月之后因为一直忙于其他事没有管理，又刚好被人注入赌博网站的暗链，导致网站被查封了。\n\n大概快一个月前，我在知乎上发布了这样一条想法:\n> \"感觉简中互联网环境下很难找到一个可以随便分享东西的平台，微信朋友圈发东西需要考虑一些微信好友的感受(浏览很多知乎问题可以看出来大家对朋友发pyq的戾气)，小红书和vb环境太差，知乎里发想法没试过，但是感觉应该也不怎么好用，其实tt是一个很合适的平台，但是感觉海外的中文互联网是一个很小的圈子大部分都是中国留学生和一些出去释放压力的非正常人完全融入不进去，而且也不想发的东西完全没人看(是的，我就是这么矛盾，想让别人看又不想让别人看)\"\n\n今天突然想到，自己搭一个网站或许是一个不错的选择，于是就有了现在的这个网站。换句话说，搭建这个网站的初衷其实不是作为一个知识分享的平台，而且作为一个个人生活分享平台，平时也许会发布一些碎碎念、一些随想随记，又或者是一些学习笔记，总而言之，这将会是一个独属于我自己的小世界，发布的内容完全随机，所以，如果某些文章内容对你造成了一些不好的影响，在此我深表歉意。\n\n我其实是一个分享欲比较强烈的人，但是从广义上来看，人生本来就是一场孤独的旅行，所以，我总是需要写点什么东西来聊以慰藉。也希望大家都能多多记录一下自己，不为被看见，只为留下痕迹。每一个当下，每一刻的感受，都是我们活着的证据。更重要的是，每一分、每一秒都是倒计时，请务必为自己而活。\n\n2023年10月19日，于湖北武汉记。"}]