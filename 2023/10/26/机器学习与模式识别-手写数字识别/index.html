<!DOCTYPE html>
<html lang=zh-CN>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:description" content="liuwx&#39;s home">
    <meta property="og:type" content="website">
    <meta name="description" content="liuwx&#39;s home">
    <meta name="keyword"  content="退役amer, 数学系在读">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        机器学习与模式识别:手写数字识别 - 刘刘大顺wx
        
    </title>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/aircloud.css">

    
<link rel="stylesheet" href="/css/gitment.css">

    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_28hi1hpxx24.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>

    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    
<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="刘刘大顺" type="application/atom+xml">
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 宁在一思进，莫在一思停。 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.png" />
        </div>
        <div class="name">
            <i>liuwx</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/collect/">
                    <i class="iconfont icon-shoucang1"></i>
                    <span>收藏</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9"><span class="toc-text">实验内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E6%96%B9%E6%B3%95"><span class="toc-text">实验数据特征提取方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0Bayes%E5%88%A4%E5%88%AB%E5%88%86%E7%B1%BB"><span class="toc-text">朴素Bayes判别分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="toc-text">理论基础</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fisher%E5%88%A4%E5%88%AB%E5%88%86%E7%B1%BB"><span class="toc-text">Fisher判别分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-text">多分类支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%BB%8B%E7%BB%8D"><span class="toc-text">二分类支持向量机介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-text">特征选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ECOC%E7%BC%96%E7%A0%81%E4%B8%8E%E5%A4%9A%E5%88%86%E7%B1%BBSVM"><span class="toc-text">ECOC编码与多分类SVM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%84%E5%88%86%E7%B1%BB%E5%99%A8%E9%97%B4%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">各分类器间的比较</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-bg" id="search-bg"></div>
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 宁在一思进，莫在一思停。 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        机器学习与模式识别:手写数字识别
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2023-10-26 12:50:58</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#Bayes" title="Bayes">Bayes</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#Fisher" title="Fisher">Fisher</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#SVM" title="SVM">SVM</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#matlab" title="matlab">matlab</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content no-indent">
        <h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><ol>
<li>数据集选择</li>
<li>Bayes判别分类</li>
<li>Fisher 线性判别</li>
<li>SVM的线性与非线性分类</li>
<li>不同分类器之间的比较</li>
</ol>
<blockquote>
<p>原始数据集上传至网盘: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1uqmJg7EGxpKR62j-Qbr1ow?pwd=jjrc">https://pan.baidu.com/s/1uqmJg7EGxpKR62j-Qbr1ow?pwd=jjrc</a></p>
</blockquote>
<h2 id="实验数据特征提取方法"><a href="#实验数据特征提取方法" class="headerlink" title="实验数据特征提取方法"></a>实验数据特征提取方法</h2><p>手写数字样本。每个数字有 $50$ 张图片，选择其中 $40$ 个作为训练集，$10$ 个作为测试集。</p>
<p>首先将含有全部特征信息的手写数字图像从坐标轴中提取出来，将提取出来的书写数字图像进行二值化处理; 将处理后的每个数字图像提取 $5\times 5$ 块模板，每个模块中 1 值像素点与总像素点的比值就是这个模块的特征值。将所有特征值放入 $5\times 5$ 的矩阵。设定阈值 $T = 0.05$，每块内所对应的元素白像素占有率大于 $T$ ，则该块特征取1;否则取0。</p>
<p>选择minst手写数字数据集，因为图片尺寸为 $28\times 28$ 不为5的倍数，使用中心裁剪法将图像裁剪为25x25的大小再进行分块操作与特征提取。</p>
<p>数据处理(特征提取) <code>dataset.m</code> 代码如下:<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% ----------</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% 数据集处理</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% ----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[train_X, train_Y, test_X, test_Y]</span> = <span class="title">load_datasets</span><span class="params">(train_pc)</span></span></span><br><span class="line">    <span class="comment">% 参数设置</span></span><br><span class="line">    T = <span class="number">0.05</span>;</span><br><span class="line">    kernelSize = <span class="number">5</span>;</span><br><span class="line">    imgSize = <span class="number">28</span>;</span><br><span class="line">    sub_counts = <span class="built_in">floor</span>(imgSize / kernelSize);</span><br><span class="line">    newImgSize = kernelSize * sub_counts;</span><br><span class="line">    train_X = [];</span><br><span class="line">    train_Y = [];</span><br><span class="line">    test_X = [];</span><br><span class="line">    test_Y = [];</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> digit = <span class="number">0</span>:<span class="number">9</span></span><br><span class="line">        digitFolderPath = fullfile(<span class="string">&#x27;./mnist&#x27;</span>, num2str(digit));</span><br><span class="line">        imageFiles = dir(fullfile(digitFolderPath, <span class="string">&#x27;*.png&#x27;</span>));</span><br><span class="line">        <span class="comment">% 读取当前数字的所有图像数据</span></span><br><span class="line">        images = <span class="built_in">length</span>(imageFiles);</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:images</span><br><span class="line">            imgPath = fullfile(digitFolderPath, imageFiles(<span class="built_in">i</span>).name);</span><br><span class="line">            <span class="comment">% 读取图像</span></span><br><span class="line">            img = double(imread(imgPath)); </span><br><span class="line">            <span class="comment">% 中心裁剪图像</span></span><br><span class="line">            croppedImg = centerCropImage(img, newImgSize);</span><br><span class="line">            <span class="comment">% 提取图像特征</span></span><br><span class="line">            features = getFeatures(croppedImg, kernelSize, T);</span><br><span class="line">            <span class="comment">%features = img(:);</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">i</span> &lt;= images * train_pc</span><br><span class="line">                <span class="comment">% 划分为训练集</span></span><br><span class="line">                train_X = [train_X; features&#x27;];</span><br><span class="line">                train_Y = [train_Y; digit];</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="comment">% 划分为测试集</span></span><br><span class="line">                test_X = [test_X; features&#x27;];</span><br><span class="line">                test_Y = [test_Y; digit];</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">croppedImg</span> = <span class="title">centerCropImage</span><span class="params">(img, newImgSize)</span></span></span><br><span class="line">        [rows, cols] = <span class="built_in">size</span>(img);</span><br><span class="line">        startRow = <span class="built_in">floor</span>((rows - newImgSize) / <span class="number">2</span>) + <span class="number">1</span>;</span><br><span class="line">        startCol = <span class="built_in">floor</span>((cols - newImgSize) / <span class="number">2</span>) + <span class="number">1</span>;</span><br><span class="line">        croppedImg = img(startRow:startRow + newImgSize - <span class="number">1</span>, startCol:startCol + newImgSize - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">features</span> = <span class="title">getFeatures</span><span class="params">(img, kernelSize, T)</span></span></span><br><span class="line">        [rows, cols] = <span class="built_in">size</span>(img);</span><br><span class="line">        <span class="comment">% 计算分块数目</span></span><br><span class="line">        numBlocksRow = <span class="built_in">round</span>(rows / kernelSize);</span><br><span class="line">        numBlocksCol = <span class="built_in">round</span>(cols / kernelSize);</span><br><span class="line">        <span class="comment">% 初始化特征向量</span></span><br><span class="line">        features = <span class="built_in">zeros</span>(<span class="number">1</span>, numBlocksRow * numBlocksCol);</span><br><span class="line">    </span><br><span class="line">        blockIndex = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:numBlocksRow</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:numBlocksCol</span><br><span class="line">                <span class="comment">% 计算分块的起始和结束位置</span></span><br><span class="line">                startRow = (<span class="built_in">i</span> - <span class="number">1</span>) * kernelSize + <span class="number">1</span>;</span><br><span class="line">                endRow = <span class="built_in">i</span> * kernelSize;</span><br><span class="line">                startCol = (<span class="built_in">j</span> - <span class="number">1</span>) * kernelSize + <span class="number">1</span>;</span><br><span class="line">                endCol = <span class="built_in">j</span> * kernelSize;</span><br><span class="line">                <span class="comment">% 计算分块的总像素数</span></span><br><span class="line">                totalPixels = kernelSize * kernelSize;</span><br><span class="line">                <span class="comment">% 计算分块内白像素的个数</span></span><br><span class="line">                whitePixels = sum(sum(img(startRow:endRow, startCol:endCol) &gt;= <span class="number">250</span>));</span><br><span class="line">                </span><br><span class="line">                <span class="comment">% 根据阈值T判断特征取值</span></span><br><span class="line">                <span class="keyword">if</span> whitePixels / totalPixels &gt; T</span><br><span class="line">                    features(blockIndex) = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    features(blockIndex) = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">                blockIndex = blockIndex + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        features = features&#x27;;</span><br><span class="line">    <span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<h2 id="朴素Bayes判别分类"><a href="#朴素Bayes判别分类" class="headerlink" title="朴素Bayes判别分类"></a>朴素Bayes判别分类</h2><h3 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h3><p>设 $B_i$ 表示事件: 图片为数字 $i$ ，则由 Bayes 公式有</p>
<script type="math/tex; mode=display">
\begin{align*}
P(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum_{j=0}^{9} P(A|B_j)P(B_j)} 
\end{align*}</script><p>其中，$P(B_i)$ 在这里是先验概率，在这里等于 0.1。$P(B_i|A)$ 是后验概率，在这里是对于一张手写数字图片(事件 $A$)上的数字是 $d$ ($0\sim 9$ 对应事件 $B_0\sim B_9$)的概率，由于这里是设计基于最小错误率的贝叶斯分类器，故而认为该数字为后验概率最大的数字。</p>
<p>令 $\mathbf{X}$ 表示图片集合，$\mathbf{Y}$ 表示标签集合，则训练数据集可以表示为:</p>
<script type="math/tex; mode=display">
\mathbf{T} = \{(\bar{x}_1, y_1), (\bar{x}_2,y_2),\cdots, (\bar{x}_n,y_n)\}</script><p>其中，$\bar{x}_1,\cdots,\bar{x}_n\in \mathbf{X}$ , $y_1,\cdots,y_n\in \mathbf{Y}$ ，对于任意的 $\bar{x}_i$ 有 $\bar{x}_i = \{x_i^1, x_i^2, \cdots, x_i^m\}$ 意为第 $i$ 张图片的 $m$ 个特征。</p>
<p>对于训练数据集 $P(X,Y)$ 独立同分布，所以有</p>
<script type="math/tex; mode=display">
P(X|Y) = \frac{P(X,Y)}{P(Y)}</script><p>又有先验概率 $P(Y = c_k) = 0.1$ , $k=0,1,\cdots,9$ , 而条件概率</p>
<script type="math/tex; mode=display">
P(X=x|Y=c_k) = P(X^1=x^1,X^2=x^2,\cdots,X^m=x^m|Y=c_k)</script><p>又因为这里数据的条件概率分布是特征条件独立，所以进一步地可以表示为</p>
<script type="math/tex; mode=display">
P(X=x|Y=c_k) = \prod_{j=1}^m P(X^j=x^j|Y=c_k)</script><blockquote>
<p>在这里问题里的实际含义是: 对于测试集的任意一张 $28\times 28$ 大小的手写数字图片，最后得到 25 个特征，每一个特征对应于每一个模块的取值。在朴素贝叶斯的假设条件下，这张图片是 1 的概率就是每一个特征都是 1 的特征的概率的累乘。</p>
</blockquote>
<p>进一步得到后验概率的计算公式:</p>
<script type="math/tex; mode=display">
P(Y=c_k|X=x) = \frac{P(Y=c_k)\prod_j^m P(X^j=x^j|Y=c_k)}{\sum_kP(Y=c_k)\prod_j^m P(X^j=x^j|Y=c_k)}</script><p>因为是要取最大值，所以可以去掉公分母，得到朴素贝叶斯分类器的判别式</p>
<script type="math/tex; mode=display">
y = f(x) = \max_{c_k} P(Y=c_k)\prod_j P(X^j=x^j|Y=c_k)</script><p>对每一个实验样本，选取前 40 个作为训练集，后 10 个作为测试集。</p>
<p>编写代码 <code>bayesClassifier.m</code><br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% ----------</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% 贝叶斯判别代码</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% ----------</span></span><br><span class="line"></span><br><span class="line">clc, clear;</span><br><span class="line"><span class="comment">% 重新加载数据</span></span><br><span class="line">load(<span class="string">&#x27;datasets.mat&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% v = mnistData&#123;8&#125;&#123;1, 2&#125;;</span></span><br><span class="line"><span class="comment">% A = reshape(v, 5, 5)&#x27;;</span></span><br><span class="line"><span class="comment">% disp(A);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 从每个类别中选择40个样本作为训练集，10个样本作为测试集</span></span><br><span class="line">train_samples = cell(<span class="number">10</span>, <span class="number">40</span>);</span><br><span class="line">test_samples = cell(<span class="number">10</span>, <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">error_count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> digit = <span class="number">1</span>:<span class="number">10</span></span><br><span class="line">    <span class="comment">% 从当前类别中随机选择40个样本作为训练集</span></span><br><span class="line">    all_samples = mnistData&#123;digit&#125;;</span><br><span class="line"><span class="comment">%     random_indices = randperm(length(all_samples), 40);</span></span><br><span class="line"><span class="comment">%     % disp(random_indices);</span></span><br><span class="line"><span class="comment">%     train_samples&#123;digit&#125; = all_samples(random_indices, :);</span></span><br><span class="line">    </span><br><span class="line">    train_samples&#123;digit&#125; = all_samples(<span class="number">1</span>:<span class="number">40</span>, :);</span><br><span class="line">    <span class="comment">% 剩余的10个样本作为测试集</span></span><br><span class="line"><span class="comment">%     test_indices = setdiff(1:length(all_samples), random_indices);</span></span><br><span class="line"><span class="comment">%     test_samples&#123;digit&#125; = all_samples(test_indices, :);</span></span><br><span class="line">    test_samples&#123;digit&#125; = all_samples(<span class="number">41</span>:<span class="number">50</span>, :);</span><br><span class="line">    <span class="comment">%(test_indices);</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 重新计算先验概率和类条件概率</span></span><br><span class="line">num_classes = <span class="number">10</span>; <span class="comment">% 数字类别数量</span></span><br><span class="line">num_features = <span class="number">25</span>; <span class="comment">% 特征数量</span></span><br><span class="line">num_train = <span class="number">40</span>; <span class="comment">% 训练样本的数量</span></span><br><span class="line">num_test = <span class="number">10</span>; <span class="comment">% 测试样本的数量</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">true</span>_positives = <span class="built_in">zeros</span>(<span class="number">1</span>, num_classes); <span class="comment">% 正类别被正确分类的样本数量</span></span><br><span class="line"><span class="built_in">false</span>_positives = <span class="built_in">zeros</span>(<span class="number">1</span>, num_classes); <span class="comment">% 负类别被错误分类成正类别的样本数量</span></span><br><span class="line"><span class="built_in">false</span>_negatives = <span class="built_in">zeros</span>(<span class="number">1</span>, num_classes); <span class="comment">% 正类别被错误分类成负类别的样本数量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x = <span class="number">1</span>:<span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> y = <span class="number">1</span>:<span class="number">10</span></span><br><span class="line">        <span class="comment">% 获取第i个测试样本的特征向量</span></span><br><span class="line">        test_sample = test_samples&#123;x&#125;&#123;y, <span class="number">2</span>&#125;;</span><br><span class="line">        </span><br><span class="line">        prior_prob = <span class="built_in">zeros</span>(<span class="number">1</span>, num_classes); <span class="comment">% 先验概率</span></span><br><span class="line">        class_cond_prob = <span class="built_in">zeros</span>(num_features, num_classes); <span class="comment">% 类条件概率</span></span><br><span class="line">        pij = []; <span class="comment">% i类的样本第j个特征为1的概率</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:num_classes</span><br><span class="line">            <span class="comment">% 计算先验概率</span></span><br><span class="line">            prior_prob(<span class="built_in">i</span>) = <span class="number">0.1</span>;</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:num_features <span class="comment">% 每个数字图片提取出来的特征数</span></span><br><span class="line">                sum = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> k = <span class="number">1</span>:num_train <span class="comment">% 每个类别下训练样本的个数</span></span><br><span class="line">                    i_feature = train_samples&#123;<span class="built_in">i</span>&#125;&#123;k, <span class="number">2</span>&#125;; <span class="comment">% 获取第k个训练样本的特征向量</span></span><br><span class="line">                    sum = sum + i_feature(<span class="built_in">j</span>);</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">                <span class="comment">% disp(sum);</span></span><br><span class="line">                pij(<span class="built_in">i</span>,<span class="built_in">j</span>) = (sum + <span class="number">1</span>) / (num_train + <span class="number">2</span>); <span class="comment">% 计算概率估计值即Pj(ωi)，注意拉普拉斯平滑处理</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:num_classes</span><br><span class="line">            multi = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:num_features <span class="comment">% 每个数字图片提取出来的特征数</span></span><br><span class="line">                <span class="keyword">if</span>(test_sample(<span class="built_in">j</span>) == <span class="number">1</span>)</span><br><span class="line">                    multi = multi * pij(<span class="built_in">i</span>,<span class="built_in">j</span>);</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    multi = multi * (<span class="number">1</span> - pij(<span class="built_in">i</span>,<span class="built_in">j</span>));</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            class_cond_prob(<span class="built_in">i</span>) = multi;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        <span class="comment">%计算后验概率</span></span><br><span class="line">        p_class = []; <span class="comment">% 后验概率</span></span><br><span class="line">        sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:num_classes<span class="comment">%数字类别个数</span></span><br><span class="line">            sum = sum + prior_prob(<span class="built_in">i</span>) * class_cond_prob(<span class="built_in">i</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:num_classes <span class="comment">% 数字类别个数</span></span><br><span class="line">            p_class(<span class="built_in">i</span>) = prior_prob(<span class="built_in">i</span>) * class_cond_prob(<span class="built_in">i</span>) / sum;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        [maxval, maxpos] = <span class="built_in">max</span>(p_class);</span><br><span class="line">        <span class="keyword">if</span> maxpos == x</span><br><span class="line">            <span class="built_in">true</span>_positives(x) = <span class="built_in">true</span>_positives(x) + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            error_count = error_count + <span class="number">1</span>;</span><br><span class="line">            <span class="built_in">false</span>_positives(maxpos) = <span class="built_in">false</span>_positives(maxpos) + <span class="number">1</span>;</span><br><span class="line">            <span class="built_in">false</span>_negatives(x) = <span class="built_in">false</span>_negatives(x) + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算准确率（acc）、精确率（precision）、召回率（recall）、F1-score</span></span><br><span class="line">precision = <span class="built_in">true</span>_positives ./ (<span class="built_in">true</span>_positives + <span class="built_in">false</span>_positives);</span><br><span class="line">recall = <span class="built_in">true</span>_positives ./ (<span class="built_in">true</span>_positives + <span class="built_in">false</span>_negatives);</span><br><span class="line">f1_score = <span class="number">2</span> * (precision .* recall) ./ (precision + recall);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算错误率和正确率</span></span><br><span class="line">error_rate = error_count / <span class="number">100</span>;</span><br><span class="line">accuracy = <span class="number">1</span> - error_rate;</span><br><span class="line"></span><br><span class="line"><span class="built_in">disp</span>([<span class="string">&#x27;Accuracy: &#x27;</span>, num2str(accuracy)]);</span><br><span class="line"><span class="built_in">disp</span>([<span class="string">&#x27;Precision: &#x27;</span>, num2str(<span class="built_in">mean</span>(precision))]); <span class="comment">% Changed here</span></span><br><span class="line"><span class="built_in">disp</span>([<span class="string">&#x27;Recall: &#x27;</span>, num2str(<span class="built_in">mean</span>(recall))]); <span class="comment">% Changed here</span></span><br><span class="line"><span class="built_in">disp</span>([<span class="string">&#x27;F1 Score: &#x27;</span>, num2str(<span class="built_in">mean</span>(f1_score))]); <span class="comment">% Changed here</span></span><br></pre></td></tr></table></figure></p>
<p>得到结果:<br>Accuracy: 0.66<br>Precision: 0.67991<br>Recall: 0.66<br>F1 Score: 0.65476</p>
<h2 id="Fisher判别分类"><a href="#Fisher判别分类" class="headerlink" title="Fisher判别分类"></a>Fisher判别分类</h2><p>使用 Fisher 线性判别方法求分类器的步骤:</p>
<ol>
<li>计算各类的均值向量: $\mu_i = \frac{1}{N_i}\sum_{x\in X_i}x$ ;</li>
<li>计算各类的类内离散矩阵: $S_{wi} = \sum_{x\in X_i}(x-\mu_i)(x-\mu_i)^T$ ;</li>
<li>计算类内总离散矩阵: $S_w = S_{w0}+S_{w1}+\cdots$ ;</li>
<li>计算总离散矩阵的逆矩阵: $S_w^{-1}$ ;</li>
<li>求出向量 $w^* = S_w^{-1}(\mu_1-\mu_0)$ ;</li>
<li>判别函数为: $y=(w^*)^Tx$ ;</li>
<li>求出判别函数的阈值: $w_0 = \frac{(w^*)^T(\mu_0+\mu_1+\cdots)}{2}$ ;</li>
<li>比较 $y$ 值与阈值的大小得出分类。</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% ----------</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Fisher分类代码</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% ----------</span></span><br><span class="line"></span><br><span class="line">clc,clear;</span><br><span class="line">load(<span class="string">&#x27;datasets.mat&#x27;</span>);</span><br><span class="line"><span class="comment">% 初始化</span></span><br><span class="line"></span><br><span class="line">numClasses = <span class="number">10</span>; <span class="comment">% 类别数</span></span><br><span class="line">numImages = <span class="number">50</span>; <span class="comment">% 每个类别的图像数</span></span><br><span class="line">numFeatures = <span class="number">25</span>; <span class="comment">% 特征数</span></span><br><span class="line">trainingSize = <span class="number">40</span>; <span class="comment">% 训练集大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 创建训练集和测试集</span></span><br><span class="line">trainingData = <span class="built_in">zeros</span>(numClasses * trainingSize, numFeatures);</span><br><span class="line">trainingLabels = <span class="built_in">zeros</span>(numClasses * trainingSize, <span class="number">1</span>);</span><br><span class="line">testData = <span class="built_in">zeros</span>(numClasses * (numImages - trainingSize), numFeatures);</span><br><span class="line">testLabels = <span class="built_in">zeros</span>(numClasses * (numImages - trainingSize), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:numClasses</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:numImages</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">j</span> &lt;= trainingSize</span><br><span class="line">            trainingData((<span class="built_in">i</span><span class="number">-1</span>)*trainingSize + <span class="built_in">j</span>, :) = mnistData&#123;<span class="built_in">i</span>,<span class="number">1</span>&#125;&#123;<span class="built_in">j</span>,<span class="number">2</span>&#125;;</span><br><span class="line">            trainingLabels((<span class="built_in">i</span><span class="number">-1</span>)*trainingSize + <span class="built_in">j</span>) = <span class="built_in">i</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            testData((<span class="built_in">i</span><span class="number">-1</span>)*(numImages - trainingSize) + <span class="built_in">j</span> - trainingSize, :) = mnistData&#123;<span class="built_in">i</span>,<span class="number">1</span>&#125;&#123;<span class="number">51</span>-<span class="built_in">j</span>,<span class="number">2</span>&#125;;</span><br><span class="line">            testLabels((<span class="built_in">i</span><span class="number">-1</span>)*(numImages - trainingSize) + <span class="built_in">j</span> - trainingSize) = <span class="built_in">i</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 使用Fisher线性判别方法进行训练</span></span><br><span class="line">MdlLinear = fitcdiscr(trainingData, trainingLabels, <span class="string">&#x27;DiscrimType&#x27;</span>, <span class="string">&#x27;pseudoLinear&#x27;</span>);</span><br><span class="line"><span class="comment">% 对测试集进行预测</span></span><br><span class="line"></span><br><span class="line">predictedLabels = predict(MdlLinear, testData);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算错误率</span></span><br><span class="line">errorRate = sum(predictedLabels ~= testLabels) / <span class="built_in">length</span>(testLabels);</span><br><span class="line">fprintf(<span class="string">&#x27;Error Rate: %.2f%%\n&#x27;</span>, errorRate * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 初始化</span></span><br><span class="line">prior = <span class="built_in">ones</span>(<span class="number">1</span>, numClasses) / numClasses; <span class="comment">% 先验概率</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 使用Fisher线性判别方法进行训练</span></span><br><span class="line">MdlLinear = fitcdiscr(trainingData, trainingLabels, <span class="string">&#x27;DiscrimType&#x27;</span>, <span class="string">&#x27;pseudoLinear&#x27;</span>, <span class="string">&#x27;Prior&#x27;</span>, prior);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 对测试集进行预测</span></span><br><span class="line">predictedLabels = predict(MdlLinear, testData);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算错误率</span></span><br><span class="line">errorRate = sum(predictedLabels ~= testLabels) / <span class="built_in">length</span>(testLabels);</span><br><span class="line">fprintf(<span class="string">&#x27;Error Rate: %.2f%%\n&#x27;</span>, errorRate * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算混淆矩阵</span></span><br><span class="line">C = confusionmat(testLabels, predictedLabels);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算准确率（accuracy）</span></span><br><span class="line">accuracy = sum(<span class="built_in">diag</span>(C)) / sum(C(:));</span><br><span class="line">fprintf(<span class="string">&#x27;Accuracy: %.2f%%\n&#x27;</span>, accuracy * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算精确率（precision）</span></span><br><span class="line">precision = <span class="built_in">diag</span>(C) ./ sum(C, <span class="number">2</span>);</span><br><span class="line">fprintf(<span class="string">&#x27;Precision: %.2f%%\n&#x27;</span>, <span class="built_in">mean</span>(precision) * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算召回率（recall）</span></span><br><span class="line">recall = <span class="built_in">diag</span>(C) ./ sum(C, <span class="number">1</span>)&#x27;;</span><br><span class="line">fprintf(<span class="string">&#x27;Recall: %.2f%%\n&#x27;</span>, <span class="built_in">mean</span>(recall) * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算F1-score</span></span><br><span class="line">f1score = <span class="number">2</span> * (precision .* recall) ./ (precision + recall);</span><br><span class="line">fprintf(<span class="string">&#x27;F1-score: %.2f%%\n&#x27;</span>, <span class="built_in">mean</span>(f1score) * <span class="number">100</span>);</span><br></pre></td></tr></table></figure>
<h2 id="多分类支持向量机"><a href="#多分类支持向量机" class="headerlink" title="多分类支持向量机"></a>多分类支持向量机</h2><h3 id="二分类支持向量机介绍"><a href="#二分类支持向量机介绍" class="headerlink" title="二分类支持向量机介绍"></a>二分类支持向量机介绍</h3><p>对于线性不可分情况引入惩罚因子 $C$ ，于是广义最优分类面问题模型如下:</p>
<script type="math/tex; mode=display">
\max_{a} \sum_{j=1}^N a_j - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N} y_iy_ja_ia_jK(x_i,x_j) , s.t.\sum_{ i=1}^N y_ia_i=0</script><p>其中 $0\le a_i\le C$ 。</p>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>用于训练SVM的特征使用的是图像的完整像素特征，即一张 $28\times 28$ 的图像，它的特征向量的大小为 $1\times 784$ 。将该特征进行标准化处理后即可用于训练SVM。</p>
<h3 id="ECOC编码与多分类SVM"><a href="#ECOC编码与多分类SVM" class="headerlink" title="ECOC编码与多分类SVM"></a>ECOC编码与多分类SVM</h3><p>ECOC(Error-Correcting Output Codes)编码是一种纠错输出编码用于将多分类任务高效地转换为多个二分类任务。Mnist数据集有0~9个数字共10分类。对应的ECOC编码如下图:</p>
<p><img src="https://raw.githubusercontent.com/wxLiu925/blog-images/master/FBxr2.png" alt="FBxr2.png"></p>
<p>代码如下:<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% ----------</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% 支持向量机代码</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% ----------</span></span><br><span class="line"></span><br><span class="line">clc,clear;</span><br><span class="line"></span><br><span class="line">[train_X, train_Y, test_X, test_Y] = load_datasets(<span class="number">0.8</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 核函数选择，可选：&#x27;linear&#x27;,&#x27;gaussian&#x27;,&#x27;rbf&#x27;,&#x27;polynomial&#x27;</span></span><br><span class="line">KernelFunction = <span class="string">&#x27;polynomial&#x27;</span>;</span><br><span class="line"><span class="comment">% 惩罚参数C确认</span></span><br><span class="line">C = <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line">template = templateSVM(...</span><br><span class="line">    <span class="string">&#x27;KernelFunction&#x27;</span>, KernelFunction, ...</span><br><span class="line">    <span class="string">&#x27;PolynomialOrder&#x27;</span>, <span class="number">3</span>, ...</span><br><span class="line">    <span class="string">&#x27;KernelScale&#x27;</span>, <span class="string">&#x27;auto&#x27;</span>, ...</span><br><span class="line">    <span class="string">&#x27;BoxConstraint&#x27;</span>, C, ...</span><br><span class="line">    <span class="string">&#x27;Standardize&#x27;</span>, <span class="built_in">true</span>);</span><br><span class="line">svm_model = fitcecoc(...</span><br><span class="line">    train_X, ...</span><br><span class="line">    train_Y, ...</span><br><span class="line">    <span class="string">&#x27;Learners&#x27;</span>, template);</span><br><span class="line"></span><br><span class="line"><span class="comment">% spy(svm_model.BinaryY(1:40:400,:));</span></span><br><span class="line"><span class="comment">% title(&#x27;ECOC编码&#x27;);</span></span><br><span class="line"><span class="comment">% yticks(1:10);</span></span><br><span class="line"><span class="comment">% yticklabels(0:9);</span></span><br><span class="line"><span class="comment">% xlabel(&#x27;分类器数目&#x27;);</span></span><br><span class="line"></span><br><span class="line">predicted_labels = predict(svm_model, test_X);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算混淆矩阵</span></span><br><span class="line">C = confusionmat(test_Y, predicted_labels);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算准确率（accuracy）</span></span><br><span class="line">accuracy = sum(<span class="built_in">diag</span>(C)) / sum(C(:));</span><br><span class="line">fprintf(<span class="string">&#x27;Accuracy: %.2f%%\n&#x27;</span>, accuracy * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算精确率（precision）</span></span><br><span class="line">precision = <span class="built_in">diag</span>(C) ./ sum(C, <span class="number">2</span>);</span><br><span class="line">fprintf(<span class="string">&#x27;Precision: %.2f%%\n&#x27;</span>, <span class="built_in">mean</span>(precision) * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算召回率（recall）</span></span><br><span class="line">recall = <span class="built_in">diag</span>(C) ./ sum(C, <span class="number">1</span>)&#x27;;</span><br><span class="line">fprintf(<span class="string">&#x27;Recall: %.2f%%\n&#x27;</span>, <span class="built_in">mean</span>(recall) * <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算F1-score</span></span><br><span class="line">f1score = <span class="number">2</span> * (precision .* recall) ./ (precision + recall);</span><br><span class="line">fprintf(<span class="string">&#x27;F1-score: %.2f%%\n&#x27;</span>, <span class="built_in">mean</span>(f1score) * <span class="number">100</span>);</span><br></pre></td></tr></table></figure></p>
<p>下表为使用全部特征进行训练、测试得到的结果。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">核函数</th>
<th style="text-align:center">惩罚参数C</th>
<th style="text-align:center">准确率(%)</th>
<th style="text-align:center">精确率(%)</th>
<th style="text-align:center">召回率(%)</th>
<th style="text-align:center">F1-score(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">线性</td>
<td style="text-align:center">1</td>
<td style="text-align:center">86.00</td>
<td style="text-align:center">86.00</td>
<td style="text-align:center">87.83</td>
<td style="text-align:center">85.77</td>
</tr>
<tr>
<td style="text-align:center">线性</td>
<td style="text-align:center">100</td>
<td style="text-align:center">82.00</td>
<td style="text-align:center">82.00</td>
<td style="text-align:center">84.35</td>
<td style="text-align:center">81.44</td>
</tr>
<tr>
<td style="text-align:center">线性</td>
<td style="text-align:center">1000</td>
<td style="text-align:center">82.00</td>
<td style="text-align:center">82.00</td>
<td style="text-align:center">84.35</td>
<td style="text-align:center">81.44</td>
</tr>
<tr>
<td style="text-align:center">高斯</td>
<td style="text-align:center">1</td>
<td style="text-align:center">71.00</td>
<td style="text-align:center">71.00</td>
<td style="text-align:center">83.30</td>
<td style="text-align:center">73.64</td>
</tr>
<tr>
<td style="text-align:center">高斯</td>
<td style="text-align:center">100</td>
<td style="text-align:center">72.00</td>
<td style="text-align:center">72.00</td>
<td style="text-align:center">82.89</td>
<td style="text-align:center">73.80</td>
</tr>
<tr>
<td style="text-align:center">高斯</td>
<td style="text-align:center">1000</td>
<td style="text-align:center">72.00</td>
<td style="text-align:center">72.00</td>
<td style="text-align:center">82.89</td>
<td style="text-align:center">73.80</td>
</tr>
<tr>
<td style="text-align:center">三次多项式</td>
<td style="text-align:center">1</td>
<td style="text-align:center">87.00</td>
<td style="text-align:center">87.00</td>
<td style="text-align:center">87.18</td>
<td style="text-align:center">86.58</td>
</tr>
<tr>
<td style="text-align:center">三次多项式</td>
<td style="text-align:center">100</td>
<td style="text-align:center">87.00</td>
<td style="text-align:center">87.00</td>
<td style="text-align:center">87.18</td>
<td style="text-align:center">86.58</td>
</tr>
<tr>
<td style="text-align:center">三次多项式</td>
<td style="text-align:center">1000</td>
<td style="text-align:center">87.00</td>
<td style="text-align:center">87.00</td>
<td style="text-align:center">87.18</td>
<td style="text-align:center">86.58</td>
</tr>
</tbody>
</table>
</div>
<p>由上表得，三次多项式作为核函数效果最佳，且乘法参数C取值对评估结果没有影响。但如果选取原始特征提取方法(图像被分为5x5个块，一共提取了25个特征)，准确率将有所下降，仅能达到 60% 左右。</p>
<h2 id="各分类器间的比较"><a href="#各分类器间的比较" class="headerlink" title="各分类器间的比较"></a>各分类器间的比较</h2><p>无论是贝叶斯判别还是Fisher分类，两者改为多分类方法比较容易。SVM是一个性能很好的二分类算法，然而在进行多分类任务时需要多个SVM才能进行，这导致SVM在多分类任务中的准确率下降。本次实验如果全部使用提取特征后的数据来训练模型，Fisher判别表现最好，准确率在74%；其次是贝叶斯判别，准确率在66%. SVM分类效果最差，准确率为61%.</p>
<p>总的来说，分类算法的选择更多取决于数据集。如果数据集规模较大，且基本线性可分，使用贝叶斯或是Fisher判别效率更高，反之应使用SVM处理更加复杂的非线性分类任务。</p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>
        <div id="lv-container"></div>
        <div class="giscus"></div>
    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        <li>
            <a target="_blank" href="https://twitter.com/DanielGrif67907">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-twitter"></i>
                            </span>
            </a>
        </li>
        
        
        <li>
            <a target="_blank" href="https://www.zhihu.com/people/li-zhi-meng-49">
                            <span class="fa-stack fa-lg">
                                 <i class="iconfont icon-zhihu"></i>
                            </span>
            </a>
        </li>
        

        

        

        
        <li>
            <a target="_blank"  href="https://github.com/wxLiu925">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-github"></i>
                            </span>
            </a>
        </li>
        

        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="#">Copyright © 2023 liuwx</a></span>
        <span>/</span>
        
        <span><a href="#">It helps SEO</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>  Theme <a target="_blank" rel="noopener" href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>

<script src="/js/index.js"></script>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




    <script src="https://giscus.app/client.js"
    data-repo="wxLiu925/blog-comments"
    data-repo-id="R_kgDOKi3_Rw"
    data-category="Announcements"
    data-category-id="DIC_kwDOKi3_R84CaTAG"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="top"
    data-theme="preferred_color_scheme"
    data-lang="zh-CN"
    data-loading="lazy"
    crossorigin="anonymous"
    async>
</script>




</html>
